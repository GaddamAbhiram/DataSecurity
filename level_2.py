# -*- coding: utf-8 -*-
"""Level-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1auTdPEn_BkaOPcD7bRLgiiul9zdobeGd
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, random_split
import matplotlib.pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def load_data(transform_train, transform_test, dataset_name='MNIST'):
    if dataset_name == 'MNIST':
        train_dataset = datasets.MNIST(
            root="./data/mnist", train=True, download=True, transform=transform_train)
        test_dataset = datasets.MNIST(
            root="./data/mnist", train=False, download=True, transform=transform_test)
    else:
        train_dataset = datasets.CIFAR10(
            root="./data/cifar-10-python", train=True, download=True, transform=transform_train)
        test_dataset = datasets.CIFAR10(
            root="./data/cifar-10-python", train=False, download=True, transform=transform_test)

    return train_dataset, test_dataset

train_transform = transforms.Compose([
    transforms.RandomRotation(10),
    transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

test_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

train_dataset, test_dataset = load_data(train_transform, test_transform, dataset_name='MNIST')

indices = torch.randperm(len(train_dataset))
shuffled_train_dataset = torch.utils.data.Subset(train_dataset, indices)

num_clients = 10
data_per_client = len(shuffled_train_dataset) // num_clients
client_datasets = random_split(shuffled_train_dataset, [data_per_client] * num_clients)

# Set num_workers=0 to avoid multiprocessing issues
test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=0)

print(f"Total training samples: {len(train_dataset)}")
print(f"Total test samples: {len(test_dataset)}")
for i, client_dataset in enumerate(client_datasets):
    print(f"Client {i + 1} dataset size: {len(client_dataset)}")

class EnhancedCNN(nn.Module):
    def __init__(self):
        super(EnhancedCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, padding=2)
        self.bn1 = nn.BatchNorm2d(32)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        self.pool = nn.MaxPool2d(2)
        self.dropout = nn.Dropout(0.2)

        dummy_input = torch.zeros(1, 1, 28, 28)
        dummy_output = self._get_conv_output(dummy_input)
        self.fc1 = nn.Linear(dummy_output, 256)
        self.bn4 = nn.BatchNorm1d(256)
        self.fc2 = nn.Linear(256, 10)

    def _get_conv_output(self, x):
        x = self.pool(torch.relu(self.bn1(self.conv1(x))))
        x = self.pool(torch.relu(self.bn2(self.conv2(x))))
        x = torch.relu(self.bn3(self.conv3(x)))
        x = self.dropout(x)
        return int(torch.flatten(x, 1).shape[1])

    def forward(self, x):
        x = self.pool(torch.relu(self.bn1(self.conv1(x))))
        x = self.pool(torch.relu(self.bn2(self.conv2(x))))
        x = torch.relu(self.bn3(self.conv3(x)))
        x = self.dropout(x)
        x = x.view(x.size(0), -1)
        x = torch.relu(self.bn4(self.fc1(x)))
        x = self.dropout(x)
        x = self.fc2(x)
        return x


def train_local_model(model, dataloader, criterion, optimizer, epochs=1):
    model.train()
    for epoch in range(epochs):
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)

            loss.backward()
            optimizer.step()

def federated_averaging(models):
    global_model = models[0]
    global_state_dict = global_model.state_dict()

    for key in global_state_dict.keys():
        global_state_dict[key] = torch.mean(
            torch.stack([model.state_dict()[key].float() for model in models]), dim=0
        )
    global_model.load_state_dict(global_state_dict)
    return global_model


def test_global_model(model, dataloader):
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    accuracy = correct / total
    print(f"Global Model Test Accuracy: {accuracy * 100:.2f}%")
    return accuracy * 100

def federated_training(num_rounds=10, num_epochs=1, batch_size=32, lr=0.001):
    global_model = EnhancedCNN().to(device)

    loss_fn = nn.CrossEntropyLoss()
    all_test_accuracies = []

    for round_num in range(num_rounds):
        print(f"--- Round {round_num + 1} ---")
        local_models = []

        for i, client_dataset in enumerate(client_datasets):
            print(f"Training on Client {i + 1}")
            local_model = EnhancedCNN().to(device)
            local_model.load_state_dict(global_model.state_dict())

            # Set num_workers=0 to avoid multiprocessing issues
            client_loader = DataLoader(client_dataset, batch_size=batch_size, shuffle=True, num_workers=0)
            optimizer = optim.Adam(local_model.parameters(), lr=lr)
            train_local_model(local_model, client_loader, loss_fn, optimizer, epochs=num_epochs)
            local_models.append(local_model)

        global_model = federated_averaging(local_models)

        test_accuracy = test_global_model(global_model, test_loader)
        all_test_accuracies.append(test_accuracy)

    return global_model, all_test_accuracies

def plot_accuracies(accuracies):
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, len(accuracies) + 1), accuracies, marker='o', linestyle='-', color='b')
    plt.title("Global Model Test Accuracy Over Rounds")
    plt.xlabel("Rounds")
    plt.ylabel("Test Accuracy (%)")
    plt.grid(True)
    plt.xticks(range(1, len(accuracies) + 1))
    plt.ylim(90, 100)
    plt.show()

if __name__ == "__main__":
    global_model, all_test_accuracies = federated_training(num_rounds=10, num_epochs=1, batch_size=32, lr=0.001)
    plot_accuracies(all_test_accuracies)
    torch.save(global_model.state_dict(), "Level2_model.pth")
    print("Model saved :)")
