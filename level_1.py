# -*- coding: utf-8 -*-
"""Level-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lfYku3QxiCjKC8mmhuQ4QLHrv1L4Tsj3
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def load_data(transform):
    train_dataset = datasets.MNIST(
        root="./data/mnist", train=True, download=True, transform=transform)
    test_dataset = datasets.MNIST(
        root="./data/mnist", train=False, download=True, transform=transform)
    return train_dataset, test_dataset

def dataloader(train_dataset, test_dataset):
    batch_size = 64
    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset), shuffle=True)
    return train_loader, test_loader

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

train_dataset, test_dataset = load_data(transform)
train_loader, test_loader = dataloader(train_dataset, test_dataset)

print(f"Total training samples: {len(train_dataset)}")
print(f"Total test samples: {len(test_dataset)}")

def cnn():
    class CNN(nn.Module):
        def __init__(self):
            super(CNN, self).__init__()
            self.conv1 = nn.Conv2d(1, 32, kernel_size=5, padding=2)
            self.bn1 = nn.BatchNorm2d(32)
            self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
            self.bn2 = nn.BatchNorm2d(64)
            self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
            self.bn3 = nn.BatchNorm2d(128)
            self.pool = nn.MaxPool2d(2)
            self.dropout = nn.Dropout(0.2)

            dummy_input = torch.zeros(1, 1, 28, 28)
            dummy_output = self._get_conv_output(dummy_input)
            self.fc1 = nn.Linear(dummy_output, 256)
            self.bn4 = nn.BatchNorm1d(256)
            self.fc2 = nn.Linear(256, 10)

        def _get_conv_output(self, x):
            x = self.pool(torch.relu(self.bn1(self.conv1(x))))
            x = self.pool(torch.relu(self.bn2(self.conv2(x))))
            x = torch.relu(self.bn3(self.conv3(x)))
            x = self.dropout(x)
            return int(torch.flatten(x, 1).shape[1])

        def forward(self, x):
            x = self.pool(torch.relu(self.bn1(self.conv1(x))))
            x = self.pool(torch.relu(self.bn2(self.conv2(x))))
            x = torch.relu(self.bn3(self.conv3(x)))
            x = self.dropout(x)
            x = x.view(x.size(0), -1)
            x = torch.relu(self.bn4(self.fc1(x)))
            x = self.dropout(x)
            x = self.fc2(x)
            return x

    return CNN()

def train_model(model, train_loader, optimizer, loss_fn, device, epochs):
    model.train()
    train_losses = []
    train_accuracies = []
    for epoch in range(epochs):
        total_loss = 0
        correct = 0
        total = 0
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = loss_fn(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        train_accuracy = 100 * correct / total
        avg_loss = total_loss / len(train_loader)
        train_losses.append(avg_loss)
        train_accuracies.append(train_accuracy)
        print(f"Epoch {epoch + 1}/{epochs} - Loss: {avg_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%")
    return train_losses, train_accuracies

def evaluate_model(model, test_loader, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    test_accuracy = 100 * correct / total
    return test_accuracy

model = cnn().to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001)
loss_fn = nn.CrossEntropyLoss()

epochs = 10
train_losses, train_accuracies = train_model(model, train_loader, optimizer, loss_fn, device, epochs)

test_accuracy = evaluate_model(model, test_loader, device)
print(f"Test Accuracy: {test_accuracy:.2f}%")

def plot_metrics(train_losses, train_accuracies, test_accuracy):
    epochs = len(train_losses)
    plt.figure(figsize=(14, 6))

    plt.subplot(1, 2, 1)
    plt.plot(range(1, epochs + 1), train_losses, marker='o', label='Training Loss')
    plt.title('Training Loss per Epoch')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.grid(True)
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(range(1, epochs + 1), train_accuracies, marker='o', label='Training Accuracy')
    plt.axhline(y=test_accuracy, color='r', linestyle='--', label='Test Accuracy')
    plt.title('Accuracy per Epoch')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy (%)')
    plt.grid(True)
    plt.legend()

    plt.tight_layout()
    plt.show()

plot_metrics(train_losses, train_accuracies, test_accuracy)

torch.save(model.state_dict(), "Level1_model.pth")
print("Model saved :)")